{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "     \n",
    "class ConvNet(object):\n",
    "    def __init__(self):\n",
    "\n",
    "        self.loss_history = []\n",
    "        self.train_acc_history = []\n",
    "        self.val_acc_history =[]\n",
    "        \n",
    "    def init_network(self, input_size, output_size):\n",
    "        h, w, c = input_size\n",
    "        model = {}\n",
    "        model['W1'] = np.random.randn(3,3,c,32)\n",
    "        model['b1'] = np.zeros((1,1,1,32))\n",
    "        model['W2'] = np.random.randn(3,3,32,64)\n",
    "        model['b2'] = np.zeros((1,1,1,64))\n",
    "        model['W3'] = np.random.randn(3,3,64,64)\n",
    "        model['b3'] = np.zeros((1,1,1,64))\n",
    "        model['W4'] = np.random.randn((h//4)*(w//4)*64,256)\n",
    "        model['b4'] = np.zeros((256))\n",
    "        model['W5'] = np.random.randn(256,output_size)\n",
    "        model['b5'] = np.zeros((output_size))\n",
    "        self.model = model\n",
    "        \n",
    "    def conv_single_step(a_slice_prev, W, b):\n",
    "        \n",
    "        s = a_slice_prev*W\n",
    "        Z = np.sum(s)\n",
    "        Z = float(Z+b)\n",
    "        \n",
    "        return Z    \n",
    "        \n",
    "    def zero_pading(self, x, pad):\n",
    "        X_pad = np.pad(X, ((0,0), (pad,pad), (pad,pad),(0,0)), mode = 'constant')\n",
    "    return X_pad\n",
    "\n",
    "\n",
    "    def conv_forward(self, A_prev, W, b, stride,pad):\n",
    "        \n",
    "        (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "        (f, f, n_C_prev, n_C) = W.shape\n",
    "        n_H = int(np.floor((n_H_prev - f + (2*pad))/stride)+1)\n",
    "        n_W = int(np.floor((n_W_prev - f + (2*pad))/stride)+1)\n",
    "        \n",
    "        Z = np.zeros([m, n_H, n_W, n_C])\n",
    "\n",
    "        A_prev_pad = zero_pad(A_prev, pad)\n",
    "\n",
    "        for i in range(m):                                  # loop over the batch of training examples\n",
    "            a_prev_pad = A_prev_pad[i]                      # Select ith training example's padded activation\n",
    "            for h in range(n_H):                           # loop over vertical axis of the output volume\n",
    "                for w in range(n_W):                       # loop over horizontal axis of the output volume\n",
    "                    for c in range(n_C):                   # loop over channels (= #filters) of the output volume\n",
    "\n",
    "                        \n",
    "                        vert_start = h * stride\n",
    "                        vert_end =vert_start+pad\n",
    "                        horiz_start = w* stride\n",
    "                        horiz_end = horiz_start+pad\n",
    "\n",
    "                        a_slice_prev = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "\n",
    "                        Z[i, h, w, c] = conv_single_step(a_slice_prev, W[...,c], b[...,c])\n",
    "\n",
    "        assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "\n",
    "        cache = (A_prev, W, b, hparameters)\n",
    "\n",
    "        return Z\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def pool_forward(self, A_prev, f,stride , mode = \"max\"):\n",
    "    \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    \n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "    \n",
    "    for i in range(m):                           # loop over the training examples\n",
    "        for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
    "            for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
    "                for c in range (n_C):            # loop over the channels of the output volume\n",
    "                    \n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    \n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    return A\n",
    "\n",
    "\n",
    "        \n",
    "    def train_step(self, x_train, y_train):\n",
    "        N,h,w,c = x_train.shape\n",
    "        scores={}\n",
    "        scores['Z1'] = self.conv_forward( x_train, self.model['W1'], self.model['b1'], 1,1)\n",
    "        scores['A1'] = np.maximum(0,scores['Z1'])\n",
    "        scores['A1'] = self.pool_forward( scores['A1'], 2,1 , mode = \"max\")\n",
    "        scores['Z2'] = self.conv_forward(scores['A1'], self.model['W2'], self.model['b2'], 1,1)\n",
    "        scores['A2'] = np.maximum(0,scores['Z2'])\n",
    "        scores['A2'] = self.pool_forward( scores['A2'], 2,1 , mode = \"max\")\n",
    "        scores['Z3'] = self.conv_forward(scores['A2'], self.model['W3'], self.model['b3'], 1,1)\n",
    "        scores['A3'] = np.maximum(0,scores['Z3'])\n",
    "        scores['A3'] = scores['A3'].reshape([-1,(h//4)*(w//4)*64])\n",
    "        scores['Z4'] = scores['A3'].dot(self.model['W4']) + self.model['b4']\n",
    "        scores['A4'] = np.maximum(0,scores['Z4'])\n",
    "        scores['Z4'] = scores['A3'].dot(self.model['W4']) + self.model['b4']\n",
    "        scores['A4'] = np.maximum(0,scores['Z4'])\n",
    "        scores['Z5'] = scores['A4'].dot(self.model['W5']) + self.model['b5']\n",
    "        \n",
    "        loss = 0\n",
    "        exp_scores = np.exp(scores['Z5'])\n",
    "        \n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "        \n",
    "        corect_logprobs = -np.log(probs[range(N), y_train])\n",
    "        loss = np.sum(corect_logprobs) / N\n",
    "        \n",
    "        grads={}\n",
    "        dscores = probs.copy()\n",
    "        dscores[range(N), list(y_train)] -= 1\n",
    "        dscores /= N\n",
    "        grads['W5'] = scores['A5'].T.dot(dscores) \n",
    "        grads['b5'] = np.sum(dscores, axis = 0)\n",
    "        grads['dZ5'] = dscores.dot(self.model['W5'].T)\n",
    "        \n",
    "        dh = grads['dZ5']\n",
    "        dh_ReLu = (scores['A4'] > 0) * dh\n",
    "        \n",
    "        grads['W4'] = scores['A4'].T.dot(dh_ReLu) \n",
    "        grads['b4'] = np.sum(dh_ReLu, axis = 0)\n",
    "        grads['dZ4'] = dscores.dot(self.model['W4'].T)\n",
    "        \n",
    "        dh = grads['dZ4']\n",
    "        dh_ReLu = (scores['A3'] > 0) * dh\n",
    "        \n",
    "        grads['W3'] = scores['A3'].T.dot(dh_ReLu) \n",
    "        grads['b3'] = np.sum(dh_ReLu, axis = 0)\n",
    "        grads['dZ4'] = dh_ReLu.dot( self.model['W4'].T)\n",
    "        \n",
    "        dh = grads['dZ5']\n",
    "        dh_ReLu = (scores['A4'] > 0) * dh\n",
    "        \n",
    "        grads['W4'] = scores['A4'].T.dot(dh_ReLu) \n",
    "        grads['b4'] = np.sum(dh_ReLu, axis = 0)\n",
    "        grads['dZ4'] = dh_ReLu.dot( self.model['W4'].T)\n",
    "                \n",
    "        \n",
    "        for i in range(len(self.model)//2-1,-1,-1):\n",
    "            \n",
    "            if i == len(self.model)//2-1:\n",
    "                grads['W' + str(i)] = scores['A'+str(i-1)].T.dot(dscores) \n",
    "                grads['b' + str(i)] = np.sum(dscores, axis = 0)\n",
    "                grads['dZ' + str(i)] = dscores.dot( self.model['W'+str(i)].T)\n",
    "            else:\n",
    "                dh = grads['dZ' + str(i+1)]\n",
    "                dh_ReLu = (scores['A'+str(i)] > 0) * dh\n",
    "                if i !=0:\n",
    "\n",
    "                    grads['W' + str(i)] = scores['A'+str(i-1)].T.dot(dh_ReLu) \n",
    "                    grads['b' + str(i)] = np.sum(dh_ReLu, axis = 0)\n",
    "                    grads['dZ' + str(i)] = dh_ReLu.dot( self.model['W'+str(i)].T)\n",
    "                else:\n",
    "                    grads['W' + str(i)] = x_train.T.dot(dh_ReLu) \n",
    "                    grads['b' + str(i)] = np.sum(dh_ReLu, axis = 0)\n",
    "                \n",
    "        return loss, grads\n",
    "    \n",
    "    def train(self, x, y, x_val=np.array([]), y_val=np.array([]), learning_rate=1e-3, num_iters=100, verbose=False):\n",
    "        \n",
    "        for step in range(1,num_iters+1):\n",
    "            loss, grads = self.train_step(x,y)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            for i in self.model:\n",
    "                self.model[i] -= grads[i]*learning_rate\n",
    "                \n",
    "            if verbose and step % 100 == 0:\n",
    "                train_acc = self.score(x,y)\n",
    "                self.train_acc_history.append(train_acc)\n",
    "                if x_val.shape[0] != 0:\n",
    "                    val_acc = self.score(x_val,y_val)\n",
    "                    self.val_acc_history.append(val_acc)\n",
    "                    print('iteration %d / %d: loss %f training accuracy %f val accuracy %f'% (step, num_iters, loss, train_acc,val_acc))\n",
    "            \n",
    "                else:\n",
    "                    print( 'iteration %d / %d: loss %f training accuracy %f'  % (step, num_iters, loss, train_acc))\n",
    "                    \n",
    "    def predict(self, x):\n",
    "        scores={}\n",
    "        for i in range(len(self.model)//2):\n",
    "            if i ==0:\n",
    "                scores['Z'+str(i)] = x.dot(self.model['W'+str(i)]) + self.model['b'+str(i)]\n",
    "                scores['A'+str(i)] = np.maximum(0,scores['Z'+str(i)])\n",
    "            else:\n",
    "                scores['Z'+str(i)] = scores['A'+str(i-1)].dot(self.model['W'+str(i)]) + self.model['b'+str(i)]\n",
    "                if i!=len(self.model)/2 -1:\n",
    "                    scores['A'+str(i)] = np.maximum(0,scores['Z'+str(i)])\n",
    "        y_pred = np.argmax(scores['Z'+str(len(self.model)//2 -1)], axis=1)\n",
    "        return y_pred\n",
    "                \n",
    "\n",
    "    def score(self,x,y):\n",
    "        pred = self.predict(x)\n",
    "        correct = pred == y\n",
    "        return np.sum(correct)/y.shape[0]\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load NNetwork.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class network_two_layer(object):\n",
    "    def __init__(self):\n",
    "        self.definition = \"two layer neular network\"\n",
    "    def init_two_layer_network(self, input_size,hidden_size, output_size):\n",
    "        model = {}\n",
    "        model['W1'] = np.random.randn(input_size,hidden_size)*0.001\n",
    "        model['b1'] = np.zeros(hidden_size)\n",
    "        model['W2'] = np.random.randn(hidden_size,output_size)*0.001\n",
    "        model['b2'] = np.zeros(output_size)\n",
    "        self.model = model\n",
    "\n",
    "    def train_step(self, x_train, y_train):\n",
    "        N, D = x_train.shape\n",
    "        Z1 = x_train.dot(self.model['W1']) + self.model['b1']\n",
    "        A1 = np.maximum(0,Z1)\n",
    "        Z2 =  A1.dot(self.model['W2']) + self.model['b2']\n",
    "        \n",
    "        \n",
    "        loss = None\n",
    "        exp_scores = np.exp(Z2)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "        \n",
    "        corect_logprobs = -np.log(probs[range(N), y_train])\n",
    "        loss = np.sum(corect_logprobs) / N\n",
    "        \n",
    "        grads={}\n",
    "        dscores = probs.copy()\n",
    "        dscores[range(N), list(y_train)] -= 1\n",
    "        dscores /= N\n",
    "        grads['W2'] = A1.T.dot(dscores) \n",
    "        grads['b2'] = np.sum(dscores, axis = 0)\n",
    "\n",
    "        dh = dscores.dot( self.model['W2'].T)\n",
    "        dh_ReLu = (A1 > 0) * dh\n",
    "        grads['W1'] = x_train.T.dot(dh_ReLu) \n",
    "        grads['b1'] = np.sum(dh_ReLu, axis = 0)\n",
    "        \n",
    "        return loss, grads\n",
    "    \n",
    "    \n",
    "    def train(self, x, y, learning_rate=1e-3, num_iters=100, verbose=False):\n",
    "        loss_history = []\n",
    "        for step in range(num_iters):\n",
    "            loss, grads = self.train_step(x,y)\n",
    "            loss_history.append(loss)\n",
    "            self.model['W2'] -= learning_rate*(grads['W2'])\n",
    "            self.model['b2'] -= learning_rate*(grads['b2'])\n",
    "            self.model['W1'] -= learning_rate*(grads['W1'])\n",
    "            self.model['b1'] -= learning_rate*(grads['b1'])\n",
    "            if verbose and step % 10 == 0:\n",
    "                print( 'iteration %d / %d: loss %f' % (step, num_iters, loss))\n",
    "        self.loss_history = loss_history\n",
    "        \n",
    "    def predict(self, x):\n",
    "        Z1 = x.dot(self.model['W1']) + self.model['b1']\n",
    "        A1 = np.maximum(0,Z1)\n",
    "        Z2 =  A1.dot(self.model['W2']) + self.model['b2']\n",
    "        y_pred = np.argmax(Z2, axis=1)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "\n",
    "class Fully_Connected(object):\n",
    "    def __init__(self):\n",
    "\n",
    "        self.loss_history = []\n",
    "        self.train_acc_history = []\n",
    "        self.val_acc_history =[]\n",
    "\n",
    "    \n",
    "    def init_layers(self, layers, sm=0.001):\n",
    "        model={}\n",
    "        for i in range(len(layers)-1):\n",
    "            model['W'+str(i)] = np.random.randn(layers[i], layers[i+1]) * sm\n",
    "            model['b'+str(i)] = np.zeros(layers[i+1])\n",
    "        self.model = model\n",
    "        \n",
    "    def init_layers_xavier(self, layers):\n",
    "        model={}\n",
    "        for i in range(len(layers)-1):\n",
    "            model['W'+str(i)] = np.random.randn(layers[i], layers[i+1])*0.1 / np.sqrt(layers[i]/2)\n",
    "            model['b'+str(i)] = np.zeros(layers[i+1])\n",
    "        self.model = model\n",
    "        \n",
    "    def train_step(self, x_train, y_train):\n",
    "        N, D = x_train.shape\n",
    "        scores={}\n",
    "        for i in range(len(self.model)//2):\n",
    "            if i ==0:\n",
    "                scores['Z'+str(i)] = x_train.dot(self.model['W'+str(i)]) + self.model['b'+str(i)]\n",
    "                scores['A'+str(i)] = np.maximum(0,scores['Z'+str(i)])\n",
    "            else:\n",
    "                scores['Z'+str(i)] = scores['A'+str(i-1)].dot(self.model['W'+str(i)]) + self.model['b'+str(i)]\n",
    "                if i!=len(self.model)/2 -1:\n",
    "                    scores['A'+str(i)] = np.maximum(0,scores['Z'+str(i)])\n",
    "        \n",
    "        loss = 0\n",
    "        exp_scores = np.exp(scores['Z'+str(len(self.model)//2 -1 )])\n",
    "        \n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "        \n",
    "        corect_logprobs = -np.log(probs[range(N), y_train])\n",
    "        loss = np.sum(corect_logprobs) / N\n",
    "        \n",
    "        grads={}\n",
    "        dscores = probs.copy()\n",
    "        dscores[range(N), list(y_train)] -= 1\n",
    "        dscores /= N\n",
    "        \n",
    "        for i in range(len(self.model)//2-1,-1,-1):\n",
    "            \n",
    "            if i == len(self.model)//2-1:\n",
    "                grads['W' + str(i)] = scores['A'+str(i-1)].T.dot(dscores) \n",
    "                grads['b' + str(i)] = np.sum(dscores, axis = 0)\n",
    "                grads['dZ' + str(i)] = dscores.dot( self.model['W'+str(i)].T)\n",
    "            else:\n",
    "                dh = grads['dZ' + str(i+1)]\n",
    "                dh_ReLu = (scores['A'+str(i)] > 0) * dh\n",
    "                if i !=0:\n",
    "\n",
    "                    grads['W' + str(i)] = scores['A'+str(i-1)].T.dot(dh_ReLu) \n",
    "                    grads['b' + str(i)] = np.sum(dh_ReLu, axis = 0)\n",
    "                    grads['dZ' + str(i)] = dh_ReLu.dot( self.model['W'+str(i)].T)\n",
    "                else:\n",
    "                    grads['W' + str(i)] = x_train.T.dot(dh_ReLu) \n",
    "                    grads['b' + str(i)] = np.sum(dh_ReLu, axis = 0)\n",
    "                \n",
    "                \n",
    "        return loss, grads\n",
    "    \n",
    "    def train(self, x, y, x_val=np.array([]), y_val=np.array([]), learning_rate=1e-3, num_iters=100, verbose=False):\n",
    "        \n",
    "        for step in range(1,num_iters+1):\n",
    "            loss, grads = self.train_step(x,y)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            for i in self.model:\n",
    "                self.model[i] -= grads[i]*learning_rate\n",
    "                \n",
    "            if verbose and step % 100 == 0:\n",
    "                train_acc = self.score(x,y)\n",
    "                self.train_acc_history.append(train_acc)\n",
    "                if x_val.shape[0] != 0:\n",
    "                    val_acc = self.score(x_val,y_val)\n",
    "                    self.val_acc_history.append(val_acc)\n",
    "                    print('iteration %d / %d: loss %f training accuracy %f val accuracy %f'% (step, num_iters, loss, train_acc,val_acc))\n",
    "            \n",
    "                else:\n",
    "                    print( 'iteration %d / %d: loss %f training accuracy %f'  % (step, num_iters, loss, train_acc))\n",
    "                    \n",
    "    def predict(self, x):\n",
    "        scores={}\n",
    "        for i in range(len(self.model)//2):\n",
    "            if i ==0:\n",
    "                scores['Z'+str(i)] = x.dot(self.model['W'+str(i)]) + self.model['b'+str(i)]\n",
    "                scores['A'+str(i)] = np.maximum(0,scores['Z'+str(i)])\n",
    "            else:\n",
    "                scores['Z'+str(i)] = scores['A'+str(i-1)].dot(self.model['W'+str(i)]) + self.model['b'+str(i)]\n",
    "                if i!=len(self.model)/2 -1:\n",
    "                    scores['A'+str(i)] = np.maximum(0,scores['Z'+str(i)])\n",
    "        y_pred = np.argmax(scores['Z'+str(len(self.model)//2 -1)], axis=1)\n",
    "        return y_pred\n",
    "                \n",
    "\n",
    "    def score(self,x,y):\n",
    "        pred = self.predict(x)\n",
    "        correct = pred == y\n",
    "        return np.sum(correct)/y.shape[0]\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
